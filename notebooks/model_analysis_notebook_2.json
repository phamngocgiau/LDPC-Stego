  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decoder Network Analysis {#decoder}\n",
    "\n",
    "Analyze the LDPC-aware decoder that extracts messages from stego images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze decoder architecture\n",
    "print(\"üîç Decoder Architecture Analysis\")\n",
    "\n",
    "decoder = model.decoder\n",
    "decoder_params = count_parameters(decoder)\n",
    "\n",
    "print(f\"Decoder parameters: {decoder_params:,}\")\n",
    "print(f\"Decoder size (MB): {decoder_params * 4 / 1024 / 1024:.2f}\")\n",
    "\n",
    "# Test decoder with stego images\n",
    "with torch.no_grad():\n",
    "    extracted_codewords = decoder(stego_images)\n",
    "\n",
    "print(f\"\\nüìè Decoder Input/Output:\")\n",
    "print(f\"  Stego images: {stego_images.shape}\")\n",
    "print(f\"  Extracted codewords: {extracted_codewords.shape}\")\n",
    "\n",
    "# Decode with LDPC system\n",
    "decoded_messages = ldpc_system.decode(extracted_codewords, attack_strength=0.3)\n",
    "print(f\"  Final decoded messages: {decoded_messages.shape}\")\n",
    "\n",
    "# Calculate message accuracy\n",
    "original_bits = (sample_message > 0.5).float()\n",
    "decoded_bits = (decoded_messages > 0.5).float()\n",
    "accuracy = (original_bits == decoded_bits).float().mean()\n",
    "print(f\"  Message accuracy: {accuracy:.1%}\")\n",
    "\n",
    "# Analyze decoder feature progression\n",
    "def analyze_decoder_features(decoder, stego_images):\n",
    "    \"\"\"Analyze how features change through decoder layers\"\"\"\n",
    "    feature_stats = []\n",
    "    \n",
    "    def hook_fn(stats_list):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, torch.Tensor) and len(output.shape) == 4:\n",
    "                stats = {\n",
    "                    'mean': output.mean().item(),\n",
    "                    'std': output.std().item(),\n",
    "                    'min': output.min().item(),\n",
    "                    'max': output.max().item(),\n",
    "                    'shape': list(output.shape)\n",
    "                }\n",
    "                stats_list.append(stats)\n",
    "        return hook\n",
    "    \n",
    "    hooks = []\n",
    "    for module in decoder.modules():\n",
    "        if isinstance(module, (nn.Conv2d, ConvBlock)):\n",
    "            hooks.append(module.register_forward_hook(hook_fn(feature_stats)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = decoder(stego_images)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return feature_stats\n",
    "\n",
    "feature_stats = analyze_decoder_features(decoder, stego_images)\n",
    "\n",
    "# Visualize feature statistics\n",
    "if feature_stats:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    layers = range(len(feature_stats))\n",
    "    means = [s['mean'] for s in feature_stats]\n",
    "    stds = [s['std'] for s in feature_stats]\n",
    "    mins = [s['min'] for s in feature_stats]\n",
    "    maxs = [s['max'] for s in feature_stats]\n",
    "    \n",
    "    ax1.plot(layers, means, 'b-o', linewidth=2, markersize=4)\n",
    "    ax1.set_title('Feature Mean Progression')\n",
    "    ax1.set_xlabel('Layer Index')\n",
    "    ax1.set_ylabel('Mean Activation')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(layers, stds, 'r-o', linewidth=2, markersize=4)\n",
    "    ax2.set_title('Feature Std Progression')\n",
    "    ax2.set_xlabel('Layer Index')\n",
    "    ax2.set_ylabel('Std Activation')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3.plot(layers, mins, 'g-o', linewidth=2, markersize=4, label='Min')\n",
    "    ax3.plot(layers, maxs, 'm-o', linewidth=2, markersize=4, label='Max')\n",
    "    ax3.set_title('Feature Range Progression')\n",
    "    ax3.set_xlabel('Layer Index')\n",
    "    ax3.set_ylabel('Activation Value')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Channel count progression\n",
    "    channels = [s['shape'][1] if len(s['shape']) >= 2 else 0 for s in feature_stats]\n",
    "    ax4.plot(layers, channels, 'c-o', linewidth=2, markersize=4)\n",
    "    ax4.set_title('Channel Count Progression')\n",
    "    ax4.set_xlabel('Layer Index')\n",
    "    ax4.set_ylabel('Number of Channels')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Decoder Feature Analysis:\")\n",
    "    print(f\"  Number of analyzed layers: {len(feature_stats)}\")\n",
    "    print(f\"  Average feature mean: {np.mean(means):.4f}\")\n",
    "    print(f\"  Average feature std: {np.mean(stds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Mechanisms {#attention}\n",
    "\n",
    "Analyze the attention mechanisms used in the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standalone attention modules for analysis\n",
    "print(\"üéØ Attention Mechanism Analysis\")\n",
    "\n",
    "# Self-attention module\n",
    "self_attn = SelfAttention(dim=256, num_heads=8)\n",
    "self_attn_params = count_parameters(self_attn)\n",
    "\n",
    "# Cross-attention module\n",
    "cross_attn = CrossAttention(query_dim=256, context_dim=128, num_heads=8)\n",
    "cross_attn_params = count_parameters(cross_attn)\n",
    "\n",
    "print(f\"Self-attention parameters: {self_attn_params:,}\")\n",
    "print(f\"Cross-attention parameters: {cross_attn_params:,}\")\n",
    "\n",
    "# Test attention modules\n",
    "batch_size = 2\n",
    "feature_size = 32\n",
    "test_features = torch.randn(batch_size, 256, feature_size, feature_size)\n",
    "test_context = torch.randn(batch_size, 128, feature_size//2, feature_size//2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Self-attention\n",
    "    self_attn_out = self_attn(test_features)\n",
    "    \n",
    "    # Cross-attention\n",
    "    cross_attn_out = cross_attn(test_features, test_context)\n",
    "\n",
    "print(f\"\\nüìè Attention Input/Output:\")\n",
    "print(f\"  Input features: {test_features.shape}\")\n",
    "print(f\"  Self-attention output: {self_attn_out.shape}\")\n",
    "print(f\"  Context features: {test_context.shape}\")\n",
    "print(f\"  Cross-attention output: {cross_attn_out.shape}\")\n",
    "\n",
    "# Analyze attention patterns\n",
    "def visualize_attention_weights(attention_module, input_tensor):\n",
    "    \"\"\"Extract and visualize attention weights\"\"\"\n",
    "    weights = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if hasattr(module, 'weight') and 'attn' in str(type(module)).lower():\n",
    "            weights.append(output.detach())\n",
    "    \n",
    "    hooks = []\n",
    "    for module in attention_module.modules():\n",
    "        if 'dropout' in str(type(module)).lower():\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = attention_module(input_tensor)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Create attention pattern visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Visualize feature maps before and after attention\n",
    "for i in range(3):\n",
    "    channel_idx = i * 85  # Sample different channels\n",
    "    \n",
    "    # Original features\n",
    "    if channel_idx < test_features.shape[1]:\n",
    "        axes[0, i].imshow(test_features[0, channel_idx].cpu().numpy(), cmap='viridis')\n",
    "        axes[0, i].set_title(f'Original Features (Ch {channel_idx})', fontsize=12)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # After self-attention\n",
    "        axes[1, i].imshow(self_attn_out[0, channel_idx].cpu().numpy(), cmap='viridis')\n",
    "        axes[1, i].set_title(f'After Self-Attention (Ch {channel_idx})', fontsize=12)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Attention Feature Transformation', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze attention statistics\n",
    "print(f\"\\nüìä Attention Analysis:\")\n",
    "print(f\"  Input feature range: [{test_features.min():.3f}, {test_features.max():.3f}]\")\n",
    "print(f\"  Self-attention output range: [{self_attn_out.min():.3f}, {self_attn_out.max():.3f}]\")\n",
    "print(f\"  Cross-attention output range: [{cross_attn_out.min():.3f}, {cross_attn_out.max():.3f}]\")\n",
    "\n",
    "# Feature correlation analysis\n",
    "input_flat = test_features.view(test_features.shape[0], -1)\n",
    "self_attn_flat = self_attn_out.view(self_attn_out.shape[0], -1)\n",
    "cross_attn_flat = cross_attn_out.view(cross_attn_out.shape[0], -1)\n",
    "\n",
    "self_corr = torch.corrcoef(torch.cat([input_flat, self_attn_flat], dim=0))[0, 1]\n",
    "cross_corr = torch.corrcoef(torch.cat([input_flat, cross_attn_flat], dim=0))[0, 1]\n",
    "\n",
    "print(f\"  Self-attention correlation with input: {self_corr:.3f}\")\n",
    "print(f\"  Cross-attention correlation with input: {cross_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recovery CVAE Analysis {#recovery}\n",
    "\n",
    "Analyze the Conditional Variational Autoencoder used for image recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and analyze Recovery CVAE\n",
    "print(\"üîÑ Recovery CVAE Analysis\")\n",
    "\n",
    "recovery_cvae = RecoveryCVAE(config)\n",
    "cvae_params = count_parameters(recovery_cvae)\n",
    "\n",
    "print(f\"Recovery CVAE parameters: {cvae_params:,}\")\n",
    "print(f\"Recovery CVAE size (MB): {cvae_params * 4 / 1024 / 1024:.2f}\")\n",
    "\n",
    "# Test CVAE with attacked images\n",
    "# Simulate attacked stego images\n",
    "attacked_stego = stego_images + torch.randn_like(stego_images) * 0.1\n",
    "\n",
    "with torch.no_grad():\n",
    "    recovered_images, mu, logvar = recovery_cvae(attacked_stego, cover_images)\n",
    "\n",
    "print(f\"\\nüìè CVAE Input/Output:\")\n",
    "print(f\"  Attacked stego: {attacked_stego.shape}\")\n",
    "print(f\"  Cover condition: {cover_images.shape}\")\n",
    "print(f\"  Recovered images: {recovered_images.shape}\")\n",
    "print(f\"  Latent mu: {mu.shape}\")\n",
    "print(f\"  Latent logvar: {logvar.shape}\")\n",
    "\n",
    "# Analyze latent space\n",
    "latent_std = torch.exp(0.5 * logvar)\n",
    "print(f\"\\nüß† Latent Space Analysis:\")\n",
    "print(f\"  Latent dimension: {mu.shape[1]}\")\n",
    "print(f\"  Mean of mu: {mu.mean():.4f}\")\n",
    "print(f\"  Std of mu: {mu.std():.4f}\")\n",
    "print(f\"  Mean of std: {latent_std.mean():.4f}\")\n",
    "print(f\"  Std of std: {latent_std.std():.4f}\")\n",
    "\n",
    "# Visualize recovery results\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"Convert tensor to displayable image\"\"\"\n",
    "    img = tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "    img = (img + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "for i in range(min(4, cover_images.shape[0])):\n",
    "    # Original cover\n",
    "    axes[0, i].imshow(tensor_to_image(cover_images[i]))\n",
    "    axes[0, i].set_title(f'Original Cover {i+1}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Attacked stego\n",
    "    axes[1, i].imshow(tensor_to_image(attacked_stego[i]))\n",
    "    axes[1, i].set_title(f'Attacked Stego {i+1}', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Recovered image\n",
    "    axes[2, i].imshow(tensor_to_image(recovered_images[i]))\n",
    "    axes[2, i].set_title(f'Recovered {i+1}', fontsize=10)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.suptitle('Recovery CVAE Results', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate recovery metrics\n",
    "from utils.helpers import calculate_psnr, calculate_ssim\n",
    "\n",
    "recovery_psnr = calculate_psnr(recovered_images, cover_images).mean()\n",
    "recovery_ssim = calculate_ssim(recovered_images, cover_images).mean()\n",
    "attack_psnr = calculate_psnr(attacked_stego, cover_images).mean()\n",
    "\n",
    "print(f\"\\nüìä Recovery Metrics:\")\n",
    "print(f\"  Attack PSNR: {attack_psnr:.2f} dB\")\n",
    "print(f\"  Recovery PSNR: {recovery_psnr:.2f} dB\")\n",
    "print(f\"  Recovery SSIM: {recovery_ssim:.4f}\")\n",
    "print(f\"  PSNR improvement: {recovery_psnr - attack_psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Profiling {#profiling}\n",
    "\n",
    "Profile the computational performance of different model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance profiling\n",
    "print(\"‚ö° Model Performance Profiling\")\n",
    "\n",
    "def profile_model_component(component, inputs, num_runs=100):\n",
    "    \"\"\"Profile execution time of a model component\"\"\"\n",
    "    component.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(inputs, tuple):\n",
    "                _ = component(*inputs)\n",
    "            else:\n",
    "                _ = component(inputs)\n",
    "    \n",
    "    # Timing\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(inputs, tuple):\n",
    "                _ = component(*inputs)\n",
    "            else:\n",
    "                _ = component(inputs)\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "    \n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times)\n",
    "    }\n",
    "\n",
    "# Profile different components\n",
    "test_batch = 1  # Use smaller batch for profiling\n",
    "profile_cover = torch.randn(test_batch, config.channels, config.image_size, config.image_size)\n",
    "profile_message = torch.randint(0, 2, (test_batch, config.message_length), dtype=torch.float32)\n",
    "profile_encoded = ldpc_system.encode(profile_message, attack_strength=0.3)\n",
    "\n",
    "print(f\"Profiling with batch size: {test_batch}\")\n",
    "print(f\"Image size: {config.image_size}x{config.image_size}\")\n",
    "\n",
    "# Profile encoder\n",
    "print(\"\\nüîß Profiling Encoder...\")\n",
    "encoder_profile = profile_model_component(model.encoder, (profile_cover, profile_encoded))\n",
    "\n",
    "# Profile decoder\n",
    "print(\"üîß Profiling Decoder...\")\n",
    "with torch.no_grad():\n",
    "    profile_stego = model.encoder(profile_cover, profile_encoded)\n",
    "decoder_profile = profile_model_component(model.decoder, profile_stego)\n",
    "\n",
    "# Profile CVAE\n",
    "print(\"üîß Profiling Recovery CVAE...\")\n",
    "cvae_profile = profile_model_component(recovery_cvae, (profile_stego, profile_cover))\n",
    "\n",
    "# Profile complete model\n",
    "print(\"üîß Profiling Complete Model...\")\n",
    "complete_profile = profile_model_component(model, (profile_cover, profile_message))\n",
    "\n",
    "# Compile results\n",
    "profiles = {\n",
    "    'Encoder': encoder_profile,\n",
    "    'Decoder': decoder_profile,\n",
    "    'Recovery CVAE': cvae_profile,\n",
    "    'Complete Model': complete_profile\n",
    "}\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Performance Results (ms per forward pass):\")\n",
    "for component, profile in profiles.items():\n",
    "    print(f\"  {component}:\")\n",
    "    print(f\"    Mean: {profile['mean_time']:.2f} ¬± {profile['std_time']:.2f} ms\")\n",
    "    print(f\"    Range: [{profile['min_time']:.2f}, {profile['max_time']:.2f}] ms\")\n",
    "\n",
    "# Visualize profiling results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Mean execution times\n",
    "components = list(profiles.keys())\n",
    "mean_times = [profiles[comp]['mean_time'] for comp in components]\n",
    "std_times = [profiles[comp]['std_time'] for comp in components]\n",
    "\n",
    "bars1 = ax1.bar(range(len(components)), mean_times, yerr=std_times, \n",
    "                capsize=5, color=sns.color_palette(\"husl\", len(components)))\n",
    "ax1.set_xlabel('Model Components')\n",
    "ax1.set_ylabel('Execution Time (ms)')\n",
    "ax1.set_title('Mean Execution Time per Component', fontsize=14, weight='bold')\n",
    "ax1.set_xticks(range(len(components)))\n",
    "ax1.set_xticklabels(components, rotation=45)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, time_val) in enumerate(zip(bars1, mean_times)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mean_times)*0.01, \n",
    "            f'{time_val:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Execution time distribution\n",
    "ax2.boxplot([profiles[comp]['mean_time'] for comp in components], \n",
    "           labels=[comp.replace(' ', '\\n') for comp in components])\n",
    "ax2.set_ylabel('Execution Time (ms)')\n",
    "ax2.set_title('Execution Time Distribution', fontsize=14, weight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate throughput\n",
    "images_per_second = 1000 / complete_profile['mean_time']\n",
    "print(f\"\\nüöÄ Throughput Analysis:\")\n",
    "print(f\"  Images per second (batch=1): {images_per_second:.2f}\")\n",
    "print(f\"  Estimated GPU memory usage: {total_params * 4 * 2 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gradient Flow Analysis {#gradients}\n",
    "\n",
    "Analyze gradient flow through the network to identify potential training issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient flow analysis\n",
    "print(\"üåä Gradient Flow Analysis\")\n",
    "\n",
    "def analyze_gradients(model, inputs, target):\n",
    "    \"\"\"Analyze gradient flow through the model\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    if isinstance(inputs, tuple):\n",
    "        outputs = model(*inputs)\n",
    "    else{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDPC Steganography - Neural Network Model Analysis\n",
    "\n",
    "This notebook provides detailed analysis of the neural network architectures used in the LDPC steganography system.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Model Architecture Overview](#architecture)\n",
    "2. [Encoder Network Analysis](#encoder)\n",
    "3. [Decoder Network Analysis](#decoder)\n",
    "4. [Attention Mechanisms](#attention)\n",
    "5. [Recovery CVAE Analysis](#recovery)\n",
    "6. [Performance Profiling](#profiling)\n",
    "7. [Gradient Flow Analysis](#gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Import model components\n",
    "from configs.ldpc_config import LDPCConfig\n",
    "from core.adaptive_ldpc import create_ldpc_system\n",
    "from models.steganography_model import AdvancedSteganographyModelWithLDPC\n",
    "from models.encoders.ldpc_aware_encoder import LDPCAwareDualUNetEncoder\n",
    "from models.decoders.ldpc_aware_decoder import LDPCAwareDualUNetDecoder\n",
    "from models.recovery.recovery_cvae import RecoveryCVAE\n",
    "from models.attention.self_attention import SelfAttention\n",
    "from models.attention.cross_attention import CrossAttention\n",
    "from models.blocks.conv_block import ConvBlock\n",
    "from models.blocks.residual_block import ResidualBlock\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üß† Neural Network Model Analysis Notebook\")\n",
    "print(\"üìö All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Architecture Overview {#architecture}\n",
    "\n",
    "Let's start by analyzing the overall architecture of our steganography system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the complete model for analysis\n",
    "print(\"üèóÔ∏è Building Complete Model Architecture\")\n",
    "\n",
    "config = LDPCConfig()\n",
    "config.device = 'cpu'\n",
    "config.image_size = 256\n",
    "config.channels = 3\n",
    "config.message_length = 1024\n",
    "config.batch_size = 4\n",
    "config.unet_base_channels = 64\n",
    "config.unet_depth = 5\n",
    "config.attention_layers = [5, 10, 15, 20]\n",
    "\n",
    "# Create LDPC system\n",
    "ldpc_system = create_ldpc_system(config)\n",
    "\n",
    "# Create complete model\n",
    "model = AdvancedSteganographyModelWithLDPC(config, ldpc_system)\n",
    "\n",
    "print(f\"‚úÖ Model created successfully!\")\n",
    "print(f\"  Image size: {config.image_size}x{config.image_size}\")\n",
    "print(f\"  Channels: {config.channels}\")\n",
    "print(f\"  Message length: {config.message_length}\")\n",
    "print(f\"  UNet depth: {config.unet_depth}\")\n",
    "print(f\"  Base channels: {config.unet_base_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def analyze_model_components(model):\n",
    "    \"\"\"Analyze parameters in each component\"\"\"\n",
    "    components = {}\n",
    "    \n",
    "    if hasattr(model, 'encoder'):\n",
    "        components['Encoder'] = count_parameters(model.encoder)\n",
    "    \n",
    "    if hasattr(model, 'decoder'):\n",
    "        components['Decoder'] = count_parameters(model.decoder)\n",
    "    \n",
    "    if hasattr(model, 'recovery_cvae'):\n",
    "        components['Recovery CVAE'] = count_parameters(model.recovery_cvae)\n",
    "    \n",
    "    if hasattr(model, 'discriminator'):\n",
    "        components['Discriminator'] = count_parameters(model.discriminator)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Analyze the complete model\n",
    "total_params = count_parameters(model)\n",
    "components = analyze_model_components(model)\n",
    "\n",
    "print(f\"üìä Model Parameter Analysis:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Total size (MB): {total_params * 4 / 1024 / 1024:.2f}\")\n",
    "\n",
    "print(f\"\\nüîß Component breakdown:\")\n",
    "for component, params in components.items():\n",
    "    percentage = (params / total_params) * 100\n",
    "    print(f\"  {component}: {params:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize parameter distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie chart\n",
    "ax1.pie(components.values(), labels=components.keys(), autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Model Parameter Distribution', fontsize=14, weight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = ax2.bar(range(len(components)), list(components.values()), \n",
    "               color=sns.color_palette(\"husl\", len(components)))\n",
    "ax2.set_xlabel('Model Components')\n",
    "ax2.set_ylabel('Number of Parameters')\n",
    "ax2.set_title('Parameter Count by Component', fontsize=14, weight='bold')\n",
    "ax2.set_xticks(range(len(components)))\n",
    "ax2.set_xticklabels(components.keys(), rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, components.values())):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(components.values())*0.01, \n",
    "            f'{value:,}', ha='center', va='bottom', fontsize=10, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder Network Analysis {#encoder}\n",
    "\n",
    "Deep dive into the LDPC-aware encoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze encoder architecture\n",
    "print(\"üîç Encoder Architecture Analysis\")\n",
    "\n",
    "encoder = model.encoder\n",
    "encoder_params = count_parameters(encoder)\n",
    "\n",
    "print(f\"Encoder parameters: {encoder_params:,}\")\n",
    "print(f\"Encoder size (MB): {encoder_params * 4 / 1024 / 1024:.2f}\")\n",
    "\n",
    "# Test encoder with sample inputs\n",
    "cover_images = torch.randn(2, config.channels, config.image_size, config.image_size)\n",
    "sample_message = torch.randint(0, 2, (2, config.message_length), dtype=torch.float32)\n",
    "\n",
    "# Encode message with LDPC first\n",
    "encoded_message = ldpc_system.encode(sample_message, attack_strength=0.3)\n",
    "\n",
    "print(f"\\nüìè Input/Output Shapes:")
print(f"  Cover images: {cover_images.shape}")
print(f"  Original messages: {sample_message.shape}")
print(f"  LDPC encoded messages: {encoded_message.shape}")

# Forward pass through encoder
with torch.no_grad():
    stego_images = encoder(cover_images, encoded_message)

print(f"  Stego images: {stego_images.shape}")

# Analyze layer-wise feature maps
def analyze_encoder_layers(encoder, cover_images, encoded_message):
    """Analyze intermediate feature maps in encoder"""
    activations = {}
    
    def hook_fn(name):
        def hook(module, input, output):
            if isinstance(output, torch.Tensor):
                activations[name] = output.detach()
        return hook
    
    # Register hooks for key layers
    hooks = []
    for name, module in encoder.named_modules():
        if isinstance(module, (nn.Conv2d, ConvBlock, ResidualBlock)):
            hooks.append(module.register_forward_hook(hook_fn(name)))
    
    # Forward pass
    with torch.no_grad():
        _ = encoder(cover_images, encoded_message)
    
    # Remove hooks
    for hook in hooks:
        hook.remove()
    
    return activations

activations = analyze_encoder_layers(encoder, cover_images, encoded_message)

print(f"\\nüî¨ Encoder Layer Analysis:")
print(f"  Number of analyzed layers: {len(activations)}")

# Visualize some key activations
layer_names = list(activations.keys())[:8]  # First 8 layers
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
axes = axes.flatten()

for i, layer_name in enumerate(layer_names):
    activation = activations[layer_name]
    if len(activation.shape) == 4:  # [B, C, H, W]
        # Show first channel of first batch
        img = activation[0, 0].cpu().numpy()
        axes[i].imshow(img, cmap='viridis')
        axes[i].set_title(f'{layer_name}\\nShape: {list(activation.shape)}', fontsize=10)
        axes[i].axis('off')

plt.suptitle('Encoder Feature Maps (First Channel)', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()